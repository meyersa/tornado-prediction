{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463ace2d-252f-43bb-ad84-4867da5caf18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Collecting aria2\n",
      "  Downloading aria2-0.0.1b0-py3-none-manylinux_2_17_x86_64.whl.metadata (28 kB)\n",
      "Collecting netCDF4\n",
      "  Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Collecting xarray\n",
      "  Downloading xarray-2024.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.8.30)\n",
      "Collecting cftime (from netCDF4)\n",
      "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.1)\n",
      "Collecting pandas>=2.1 (from xarray)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.1->xarray)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.1->xarray)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n",
      "Downloading aria2-0.0.1b0-py3-none-manylinux_2_17_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2024.11.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, scipy, joblib, cftime, aria2, scikit-learn, pandas, netCDF4, xarray\n",
      "Successfully installed aria2-0.0.1b0 cftime-1.6.4.post1 joblib-1.4.2 netCDF4-1.7.2 pandas-2.2.3 pytz-2024.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0 tqdm-4.67.1 tzdata-2024.2 xarray-2024.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests tensorflow[and-cuda] aria2 netCDF4 numpy xarray scikit-learn tqdm pickleshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1826bbf-3b5e-4881-8d5a-fb5e1cf07799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Constants\n",
    "DOWNLOAD_DATA = True\n",
    "DATA_DIR = './data'  # Directory containing .tar.gz files\n",
    "EXTRACT_DIR = os.path.join(DATA_DIR, 'extracted')\n",
    "TRAIN_DIR = \"./data/extracted/train\"\n",
    "TEST_DIR = \"./data/extracted/test\"\n",
    "TRAIN_OUTPUT_DIR = \"./data/tfrecords/train\"\n",
    "TEST_OUTPUT_DIR = \"./data/tfrecords/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c3372-4fa2-4c9c-b93d-588fc1f16403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# Bucket and endpoint configuration\n",
    "CUSTOM_ENDPOINT = \"bbproxy.meyerstk.com/file\"\n",
    "APP = \"TorNetBecauseZenodoSlow\"\n",
    "TMP_FILE = os.path.join(DATA_DIR, \"tmp.txt\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "def download_links(links):\n",
    "    \"\"\"\n",
    "    Download files from the provided links using aria2c.\n",
    "    Uses a file named tmp.txt in DATA_DIR for links.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Write links to tmp.txt\n",
    "        with open(TMP_FILE, 'w') as file:\n",
    "            file.writelines(link + '\\n' for link in links)\n",
    "        logging.info(f\"Temporary file created: {TMP_FILE}\")\n",
    "\n",
    "        # Run aria2c to download files\n",
    "        logging.info(f\"Starting downloads for links: {', '.join(links)}\")\n",
    "        command = [\n",
    "            \"aria2c\",\n",
    "            \"-j\", \"5\",                # Download up to 3 files concurrently\n",
    "            \"-x\", \"16\",               # Use up to 16 connections per file\n",
    "            # \"--console-log-level=info\",\n",
    "            \"-s\", \"16\",               # Split each file into 16 segments\n",
    "            \"--dir\", DATA_DIR,        # Specify the download directory\n",
    "            \"-i\", TMP_FILE            # Input file with download links\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        logging.info(\"Downloads completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during download: {e}\")\n",
    "        exit(1)\n",
    "    finally:\n",
    "        if os.path.exists(TMP_FILE):\n",
    "            os.remove(TMP_FILE)\n",
    "            logging.info(f\"Temporary file deleted: {TMP_FILE}\")\n",
    "\n",
    "\n",
    "def download_files_with_aria():\n",
    "    \"\"\"\n",
    "    Download files from a public Backblaze B2 bucket served via a custom endpoint using aria2c.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting download process with aria2c...\")\n",
    "\n",
    "    # # List of files to download\n",
    "    file_list = [\n",
    "        \"tornet_2013.tar.gz\",\n",
    "        \"tornet_2014.tar.gz\",\n",
    "        \"tornet_2015.tar.gz\",\n",
    "        \"tornet_2016.tar.gz\",\n",
    "        \"tornet_2017.tar.gz\",\n",
    "        \"tornet_2018.tar.gz\",\n",
    "        \"tornet_2019.tar.gz\",\n",
    "        \"tornet_2020.tar.gz\",\n",
    "        \"tornet_2021.tar.gz\",\n",
    "        \"tornet_2022.tar.gz\",\n",
    "    ]\n",
    "\n",
    "    # Construct the public URLs\n",
    "    links = [f\"https://{CUSTOM_ENDPOINT}/{APP}/{file_name}\" for file_name in file_list]\n",
    "    \n",
    "    # Filter out already downloaded files\n",
    "    links_to_download = [\n",
    "        link for link in links\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, os.path.basename(link)))\n",
    "    ]\n",
    "\n",
    "    if links_to_download:\n",
    "        download_links(links_to_download)\n",
    "    else:\n",
    "        logging.info(\"All files already downloaded.\")\n",
    "\n",
    "\n",
    "def extract_local_tar_files():\n",
    "    \"\"\"\n",
    "    Extract all .tar.gz files from the local DATA_DIR to EXTRACT_DIR.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting extraction process...\")\n",
    "    for file_name in os.listdir(DATA_DIR):\n",
    "        if file_name.endswith('.tar.gz'):\n",
    "            file_path = os.path.join(DATA_DIR, file_name)\n",
    "            logging.info(f'Extracting {file_path}...')\n",
    "            with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                tar.extractall(path=EXTRACT_DIR)\n",
    "            logging.info(f'Extracted {file_path} to {EXTRACT_DIR}')\n",
    "\n",
    "            os.remove(file_path)\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "    download_files_with_aria()\n",
    "    extract_local_tar_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c75a6f-cec6-41a6-80ad-ed1ab246f36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training TFRecords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2013: 100%|██████████| 3498/3498 [02:36<00:00, 22.38it/s]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2013: 3498 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2018: 100%|██████████| 15360/15360 [11:21<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2018: 15360 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2017: 100%|██████████| 16927/16927 [12:31<00:00, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2017: 16927 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2014: 100%|██████████| 17447/17447 [12:56<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2014: 17447 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2020: 100%|██████████| 18147/18147 [13:28<00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2020: 18147 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2016: 100%|██████████| 18791/18791 [13:57<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2016: 18791 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2021: 100%|██████████| 19049/19049 [14:15<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2021: 19049 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2015: 100%|██████████| 19721/19721 [14:39<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2015: 19721 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2019: 100%|██████████| 20588/20588 [15:19<00:00, 22.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2019: 20588 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2022: 100%|██████████| 22138/22138 [16:34<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2022: 22138 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all years: 100%|██████████| 10/10 [16:34<00:00, 99.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating testing TFRecords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2013: 100%|██████████| 573/573 [00:25<00:00, 22.34it/s]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2013: 573 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2018: 100%|██████████| 2518/2518 [01:51<00:00, 22.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2018: 2518 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2014: 100%|██████████| 2546/2546 [01:54<00:00, 22.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2014: 2546 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2022: 100%|██████████| 2777/2777 [02:04<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2022: 2777 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2016: 100%|██████████| 2951/2951 [02:11<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2016: 2951 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2017: 100%|██████████| 3145/3145 [02:20<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2017: 3145 files"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2021:  73%|███████▎  | 3119/4268 [02:20<00:52, 21.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2015: 100%|██████████| 3902/3902 [02:53<00:00, 22.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2015: 3902 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2019: 100%|██████████| 4031/4031 [03:00<00:00, 22.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2019: 4031 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2021: 100%|██████████| 4268/4268 [03:12<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2021: 4268 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing year 2020: 100%|██████████| 4756/4756 [03:31<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 2020: 4756 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all years: 100%|██████████| 10/10 [03:31<00:00, 21.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Label Counts: Counter({0: 189276, 1: 13857})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Constants for normalization\n",
    "CHANNEL_MIN_MAX = {\n",
    "    'DBZ': [-20., 60.],\n",
    "    'VEL': [-60., 60.],\n",
    "    'KDP': [-2., 5.],\n",
    "    'RHOHV': [0.2, 1.04],\n",
    "    'ZDR': [-1., 8.],\n",
    "    'WIDTH': [0., 9.]\n",
    "}\n",
    "\n",
    "VARIABLES = ['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH']\n",
    "\n",
    "def parse_nc_file(file_path):\n",
    "    \"\"\"\n",
    "    Parse and preprocess a single .nc file.\n",
    "    Output: features (4D array), label (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, engine=\"netcdf4\") as ds:\n",
    "            data_list = []\n",
    "\n",
    "            # Process radar variables\n",
    "            for var in VARIABLES:\n",
    "                if var not in ds:\n",
    "                    raise ValueError(f\"Variable {var} not found in dataset.\")\n",
    "\n",
    "                var_data = ds[var].values  # Shape: [time, azimuth, range, sweep]\n",
    "                var_min, var_max = CHANNEL_MIN_MAX[var]\n",
    "\n",
    "                # Handle missing data and normalize\n",
    "                var_data = np.nan_to_num(var_data, nan=0, posinf=0, neginf=0)\n",
    "                var_data[var_data == ds.attrs.get('MissingDataFlag', -999.0)] = 0\n",
    "                var_data = np.clip(var_data, var_min, var_max)\n",
    "                var_data = (var_data - var_min) / (var_max - var_min)\n",
    "                var_data = (var_data * 255).astype(np.uint8)  # Scale to [0, 255] and convert to uint8\n",
    "\n",
    "                data_list.append(var_data)\n",
    "\n",
    "            # Combine variables into the channel dimension\n",
    "            data = np.stack(data_list, axis=-1)  # Shape: [time, azimuth, range, sweep, variables]\n",
    "            data = data.transpose(0, 1, 2, 4, 3)  # [time, azimuth, range, variables, sweep]\n",
    "            data = data.reshape(data.shape[0], data.shape[1], data.shape[2], -1)  # [time, azimuth, range, channels]\n",
    "\n",
    "            # Ensure correct time dimension\n",
    "            if data.shape[0] < 4:\n",
    "                raise ValueError(f\"File {file_path} has fewer than 4 time steps.\")\n",
    "\n",
    "            # Extract label from category attribute\n",
    "            label = ds.attrs.get(\"category\", \"NUL\")\n",
    "            label = 1 if label == \"TOR\" else 0\n",
    "\n",
    "            return data[:4], label  # Return first 4 time steps\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def serialize_example(features, label):\n",
    "    \"\"\"\n",
    "    Serialize features and labels into a TFRecord-compatible format.\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        \"features\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[features.tobytes()])),\n",
    "        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def group_files_by_year(input_dir):\n",
    "    \"\"\"\n",
    "    Group `.nc` files by year.\n",
    "    \"\"\"\n",
    "    files_by_year = defaultdict(list)\n",
    "    \n",
    "    for file in Path(input_dir).rglob(\"*.nc\"):\n",
    "        year = file.parent.name  # Assuming year is the folder name\n",
    "        files_by_year[year].append(file)\n",
    "\n",
    "    return files_by_year\n",
    "\n",
    "def process_year(year, files, output_dir):\n",
    "    \"\"\"\n",
    "    Process files for a given year and save them as a TFRecord file.\n",
    "    \"\"\"\n",
    "    output_path = str(Path(output_dir) / f\"{year}.tfrecord\")\n",
    "    local_label_counts = Counter()  # Local Counter for this process\n",
    "    \n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        for file in tqdm(files, desc=f\"Processing year {year}\"):\n",
    "            features, label = parse_nc_file(file)\n",
    "            if features is not None:\n",
    "                example = serialize_example(features, label)\n",
    "                writer.write(example)\n",
    "                local_label_counts[label] += 1  # Update local counts\n",
    "\n",
    "    print(f\"Completed {year}: {len(files)} files\")\n",
    "    return local_label_counts \n",
    "    \n",
    "def create_tfrecords(input_dir, output_dir, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create TFRecords for all years in train/test directories in parallel.\n",
    "    Aggregate label counts from all processes.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files_by_year = group_files_by_year(input_dir)\n",
    "\n",
    "    total_label_counts = Counter()  # Global Counter for all processes\n",
    "\n",
    "    # Process each year in parallel\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_year, year, files, output_dir)\n",
    "            for year, files in files_by_year.items()\n",
    "        ]\n",
    "        for future in tqdm(futures, desc=\"Processing all years\"):\n",
    "            year_label_counts = future.result()  # Get label counts from process\n",
    "            total_label_counts.update(year_label_counts)  # Aggregate counts\n",
    "    \n",
    "    return total_label_counts\n",
    "\n",
    "# Create TFRecords\n",
    "print(\"Creating training TFRecords...\")\n",
    "train_counts = create_tfrecords(TRAIN_DIR, TRAIN_OUTPUT_DIR, num_workers=50)\n",
    "\n",
    "print(\"Creating testing TFRecords...\")\n",
    "test_counts = create_tfrecords(TEST_DIR, TEST_OUTPUT_DIR, num_workers=50)\n",
    "\n",
    "total_label_counts = train_counts + test_counts\n",
    "print(f\"Total Label Counts: {total_label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e34fc79-ac90-40da-a09b-2a8ac9a911b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'total_label_counts' (Counter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/extensions/storemagic.py:229: UserWarning: This is now an optional IPython functionality, setting autorestore/total_label_counts requires you to install the `pickleshare` library.\n",
      "  db[ 'autorestore/' + arg ] = obj\n"
     ]
    }
   ],
   "source": [
    "%store total_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1baed0-8311-446f-9e6c-9fcebbbc49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "total_label_counts = counter = Counter({0: 189276, 1: 13857})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba86bd85-082b-4e14-9560-0892836d98bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:06:42.097517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733436402.119746     486 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733436402.126465     486 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 22:06:42.149572: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733436404.873290     486 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22282 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1733436404.874090     486 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22282 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:81:00.0, compute capability: 8.9\n",
      "I0000 00:00:1733436404.874757     486 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22282 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c1:00.0, compute capability: 8.9\n",
      "I0000 00:00:1733436404.875455     486 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22282 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c2:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# Create a MirroredStrategy for multiple GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347cc128-7e0c-4647-831b-82aa74ef4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:18:08.931159: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 171666 (Steps: 10730), Test samples: 31467 (Steps: 1967)\n",
      "Feature shape: (64, 4, 120, 240, 12), Label shape: (64, 1)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np \n",
    "\n",
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        \"features\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(example, feature_description)\n",
    "\n",
    "    # Decode features and reshape directly to the known fixed shape\n",
    "    features = tf.io.decode_raw(parsed_example[\"features\"], tf.uint8)\n",
    "    features = tf.reshape(features, [4, 120, 240, 12])  # Directly use the fixed shape\n",
    "    features = tf.cast(features, tf.float16) / 255.0  # Scale back to [0, 1]\n",
    "\n",
    "    # Parse label\n",
    "    label = tf.cast(parsed_example[\"label\"], tf.float32)\n",
    "    label = tf.reshape(label, (1,))  # Ensure label has shape [1]\n",
    "\n",
    "    return features, label\n",
    "\n",
    "def create_tf_dataset_with_count(tfrecord_dir, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset from TFRecord files and count total samples.\n",
    "    \"\"\"\n",
    "    tfrecord_files = list(Path(tfrecord_dir).glob(\"*.tfrecord\"))\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
    "\n",
    "    sample_count = 0\n",
    "    for record in dataset:\n",
    "        sample_count += 1\n",
    "\n",
    "    dataset = dataset.map(parse_tfrecord)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    batch_size *= strategy.num_replicas_in_sync\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    return dataset, sample_count\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, train_sample_count = create_tf_dataset_with_count(TRAIN_OUTPUT_DIR, batch_size=BATCH_SIZE)\n",
    "test_dataset, test_sample_count = create_tf_dataset_with_count(TEST_OUTPUT_DIR, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_steps_per_epoch = np.ceil(train_sample_count / BATCH_SIZE).astype(int)\n",
    "validation_steps = np.ceil(test_sample_count / BATCH_SIZE).astype(int)\n",
    "\n",
    "print(f\"Train samples: {train_sample_count} (Steps: {train_steps_per_epoch}), Test samples: {test_sample_count} (Steps: {validation_steps})\")\n",
    "\n",
    "for features, labels in train_dataset.take(1):\n",
    "    print(f\"Feature shape: {features.shape}, Label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db146851-3d91-4769-8d35-3f9c4b4e5370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: np.float64(0.5366052748367464), 1: np.float64(7.329616800173198)}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "def create_3d_torcnn(input_shape=(4, 120, 240, 12), dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Define a 3D CNN model for tornado detection with advanced optimizations.\n",
    "    \"\"\"\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            # Input Layer\n",
    "            layers.Input(shape=input_shape),\n",
    "            \n",
    "            # Block 1\n",
    "            layers.Conv3D(32, (3, 3, 3), padding=\"same\", kernel_regularizer=l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.Conv3D(32, (3, 3, 3), padding=\"same\", kernel_regularizer=l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.MaxPooling3D((1, 2, 2)),\n",
    "            layers.SpatialDropout3D(dropout_rate),\n",
    "\n",
    "            # Block 2\n",
    "            layers.Conv3D(64, (3, 3, 3), padding=\"same\", kernel_regularizer=l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.Conv3D(64, (3, 3, 3), padding=\"same\", kernel_regularizer=l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.MaxPooling3D((1, 2, 2)),\n",
    "            layers.SpatialDropout3D(dropout_rate),\n",
    "\n",
    "            # Block 3\n",
    "            layers.Conv3D(128, (3, 3, 3), padding=\"same\", kernel_regularizer=l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.Conv3D(128, (3, 3, 3), padding=\"same\", kernel_regularizer=l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.MaxPooling3D((2, 2, 2)),\n",
    "            layers.SpatialDropout3D(dropout_rate),\n",
    "\n",
    "            # Global Pooling\n",
    "            layers.GlobalMaxPooling3D(),\n",
    "\n",
    "            # Fully Connected Layers\n",
    "            layers.Dense(128, kernel_regularizer=l2(0.01)),\n",
    "            layers.LeakyReLU(negative_slope=0.1),\n",
    "            layers.Dropout(0.1),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Tornado (1), Storm (0)\n",
    "labels = [0, 1]\n",
    "counts = [total_label_counts[0], total_label_counts[1]]\n",
    "\n",
    "# Convert classes to a NumPy array\n",
    "classes = np.array(labels)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=np.repeat(classes, counts))\n",
    "\n",
    "# Convert to dictionary for TensorFlow\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(f\"Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# Help with VRAM\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.backend import clear_session\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "clear_session()\n",
    "\n",
    "# TF Logging \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf97164-aa78-43db-a230-6d76f5ef65f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,680</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout3d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout3D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout3d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout3D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout3d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout3D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling3d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling3D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,    │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,    │        \u001b[38;5;34m27,680\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,    │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m240\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m32\u001b[0m)                    │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout3d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout3D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m55,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m) │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m) │       \u001b[38;5;34m110,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m) │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout3d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout3D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_4 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m) │       \u001b[38;5;34m221,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m) │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3d_5 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m) │       \u001b[38;5;34m442,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m) │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling3d_2 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout3d_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout3D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling3d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling3D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">886,337</span> (3.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m886,337\u001b[0m (3.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">885,441</span> (3.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m885,441\u001b[0m (3.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build and compile the model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = create_3d_torcnn()\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005, clipvalue=1.0),  # Gradient clipping\n",
    "        loss=BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\"accuracy\", Precision(name=\"precision\"), Recall(name=\"recall\"), AUC(name=\"auc\")]\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051e2c-2a26-4535-b61e-906cb43250ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:20:05,902 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:20:06,171 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:20:06,174 - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "INFO:tensorflow:Collective all_reduce tensors: 28 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:20:09,318 - INFO - Collective all_reduce tensors: 28 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 3928/10730\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m17:45\u001b[0m 157ms/step - accuracy: 0.6447 - auc: 0.7333 - loss: 1.5351 - precision: 0.1363 - recall: 0.7087"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "webhook_url = \"https://discord.com/api/webhooks/1314273027344830476/OeYuqZq6RDAnEKKg_LSC6WXv1YL0Kdd1YenEsjCLyFdqjJA8aOQwDta44HsvoNfmD6Jl\"\n",
    "class DiscordWebhookCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, webhook_url):\n",
    "        super().__init__()\n",
    "        self.webhook_url = webhook_url\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Prepare the embed for Discord\n",
    "        embed = {\n",
    "            \"title\": f\"Epoch {epoch + 1} Results\",\n",
    "            \"description\": (\n",
    "                \"The model has completed an epoch. Here are the key metrics:\\n\\n\"\n",
    "                f\"**Metrics:**\\n\"\n",
    "                f\"- Training Loss: {logs.get('loss'):.4f}\\n\"\n",
    "                f\"- Training Accuracy: {logs.get('accuracy'):.4f}\\n\"\n",
    "                f\"- Validation Loss: {logs.get('val_loss'):.4f}\\n\"\n",
    "                f\"- Validation Accuracy: {logs.get('val_accuracy'):.4f}\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # Create the payload for the webhook\n",
    "        payload = {\n",
    "            \"content\": f\"Epoch {epoch + 1} complete.\",\n",
    "            \"embeds\": [embed]\n",
    "        }\n",
    "        \n",
    "checkpoint_dir = \"/modelweights\"\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=15,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=test_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weight_dict,  # Apply class weights\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"{checkpoint_dir}/best.model.keras\",\n",
    "            monitor='val_loss',  # Monitor the validation loss\n",
    "            save_best_only=True,  # Save only the best model\n",
    "            save_weights_only=False,  # Save the entire model (weights + architecture)\n",
    "            mode='min',  # Save when 'val_loss' is minimized\n",
    "            verbose=1  # Print messages when saving\n",
    "        ),\n",
    "        DiscordWebhookCallback(webhook_url),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save(f\"{checkpoint_dir}/final.model.h5\")\n",
    "print(f\"Final model saved at: {final_model_filepath}/final.model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0c0931-e82f-4f31-9d05-a94611137e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot Metrics (e.g., Precision, Recall, AUC)\n",
    "plt.plot(history.history['precision'], label='Training Precision')\n",
    "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title('Training vs. Validation Precision')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['recall'], label='Training Recall')\n",
    "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.title('Training vs. Validation Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed3458-6246-4224-a4d4-dbb1bfc59915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
