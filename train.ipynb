{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463ace2d-252f-43bb-ad84-4867da5caf18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: aria2 in /opt/conda/lib/python3.11/site-packages (0.0.1b0)\n",
      "Requirement already satisfied: netCDF4 in /opt/conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: xarray in /opt/conda/lib/python3.11/site-packages (2024.11.0)\n",
      "Requirement already satisfied: dask in /opt/conda/lib/python3.11/site-packages (2024.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: distributed in /opt/conda/lib/python3.11/site-packages (2024.12.0)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.18.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: cftime in /opt/conda/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/conda/lib/python3.11/site-packages (from xarray) (24.1)\n",
      "Requirement already satisfied: pandas>=2.1 in /opt/conda/lib/python3.11/site-packages (from xarray) (2.2.3)\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.11/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from dask) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.11/site-packages (from dask) (2024.10.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.11/site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /opt/conda/lib/python3.11/site-packages (from dask) (8.5.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /opt/conda/lib/python3.11/site-packages (from distributed) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from distributed) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /opt/conda/lib/python3.11/site-packages (from distributed) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from distributed) (6.1.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /opt/conda/lib/python3.11/site-packages (from distributed) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from distributed) (6.4.1)\n",
      "Requirement already satisfied: zict>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (5.29.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata>=4.13.0->dask) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2>=2.10.3->distributed) (3.0.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests aria2 netCDF4 numpy xarray tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1826bbf-3b5e-4881-8d5a-fb5e1cf07799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "DOWNLOAD_DATA = True\n",
    "DATA_DIR = './data'  # Directory containing .tar.gz files\n",
    "# Temporary file for download links\n",
    "TMP_FILE = os.path.join(DATA_DIR, 'tmp.txt')\n",
    "EXTRACT_DIR = os.path.join(DATA_DIR, 'extracted')\n",
    "\n",
    "# Bucket and endpoint configuration\n",
    "CUSTOM_ENDPOINT = \"bbproxy.meyerstk.com/file\"\n",
    "APP = \"TorNetBecauseZenodoSlow\"  # Bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118c3372-4fa2-4c9c-b93d-588fc1f16403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:45:37,593 - INFO - Starting download process with aria2c...\n",
      "2024-12-04 17:45:37,593 - INFO - Temporary file created: ./data/tmp.txt\n",
      "2024-12-04 17:45:37,594 - INFO - Starting downloads for links: https://bbproxy.meyerstk.com/file/TorNetBecauseZenodoSlow/catalog.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12/04 17:45:37 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:45:41,753 - INFO - Downloads completed successfully.\n",
      "2024-12-04 17:45:41,754 - INFO - Temporary file deleted: ./data/tmp.txt\n",
      "2024-12-04 17:45:41,754 - INFO - Starting extraction process...\n",
      "2024-12-04 17:45:41,755 - INFO - Extracting ./data/tornet_2013.tar.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#7efeee 0B/0B CN:1 DL:0B]\n",
      "[#7efeee 1.2MiB/36MiB(3%) CN:1 DL:1.9MiB ETA:18s]\n",
      "[#7efeee 17MiB/36MiB(46%) CN:1 DL:10MiB ETA:1s]\n",
      "[#7efeee 35MiB/36MiB(98%) CN:1 DL:13MiB]\n",
      "\n",
      "12/04 17:45:41 [\u001b[1;32mNOTICE\u001b[0m] Download complete: ./data/catalog.csv\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "7efeee|OK  |    13MiB/s|./data/catalog.csv\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 17:45:54,574 - INFO - Extracted ./data/tornet_2013.tar.gz to ./data/extracted\n",
      "2024-12-04 17:45:54,575 - INFO - Extracting ./data/tornet_2014.tar.gz...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m     download_files_with_aria()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Call the function to process the local .tar.gz files\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[43mextract_local_tar_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 106\u001b[0m, in \u001b[0;36mextract_local_tar_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXTRACT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXTRACT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:2294\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner, filter)\u001b[0m\n\u001b[1;32m   2289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2290\u001b[0m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m         directories\u001b[38;5;241m.\u001b[39mappend(tarinfo)\n\u001b[0;32m-> 2294\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m directories\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m a: a\u001b[38;5;241m.\u001b[39mname, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:2357\u001b[0m, in \u001b[0;36mTarFile._extract_one\u001b[0;34m(self, tarinfo, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_fatal_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:2440\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, tarinfo\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   2439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[0;32m-> 2440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:2494\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2492\u001b[0m     target\u001b[38;5;241m.\u001b[39mtruncate()\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2494\u001b[0m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mReadError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/tarfile.py:258\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    255\u001b[0m     dst\u001b[38;5;241m.\u001b[39mwrite(buf)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remainder \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 258\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremainder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m remainder:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread(size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/gzip.py:507\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    505\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 507\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_links(links):\n",
    "    \"\"\"\n",
    "    Download files from the provided links using aria2c.\n",
    "    Uses a file named tmp.txt in DATA_DIR for links.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Write links to tmp.txt\n",
    "        with open(TMP_FILE, 'w') as file:\n",
    "            file.writelines(link + '\\n' for link in links)\n",
    "        logging.info(f\"Temporary file created: {TMP_FILE}\")\n",
    "\n",
    "        # Run aria2c to download files\n",
    "        logging.info(f\"Starting downloads for links: {', '.join(links)}\")\n",
    "        command = [\n",
    "            \"aria2c\",\n",
    "            \"-j\", \"5\",                # Download up to 3 files concurrently\n",
    "            \"-x\", \"16\",               # Use up to 16 connections per file\n",
    "            # \"--console-log-level=info\",\n",
    "            \"-s\", \"16\",               # Split each file into 16 segments\n",
    "            \"--dir\", DATA_DIR,        # Specify the download directory\n",
    "            \"-i\", TMP_FILE            # Input file with download links\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        logging.info(\"Downloads completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during download: {e}\")\n",
    "        exit(1)\n",
    "    finally:\n",
    "        if os.path.exists(TMP_FILE):\n",
    "            os.remove(TMP_FILE)\n",
    "            logging.info(f\"Temporary file deleted: {TMP_FILE}\")\n",
    "\n",
    "\n",
    "def download_files_with_aria():\n",
    "    \"\"\"\n",
    "    Download files from a public Backblaze B2 bucket served via a custom endpoint using aria2c.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting download process with aria2c...\")\n",
    "\n",
    "    # # List of files to download\n",
    "    file_list = [\n",
    "        \"tornet_2013.tar.gz\",\n",
    "        \"tornet_2014.tar.gz\",\n",
    "        \"tornet_2015.tar.gz\",\n",
    "        \"tornet_2016.tar.gz\",\n",
    "        \"tornet_2017.tar.gz\",\n",
    "        \"tornet_2018.tar.gz\",\n",
    "        \"tornet_2019.tar.gz\",\n",
    "        \"tornet_2020.tar.gz\",\n",
    "        \"tornet_2021.tar.gz\",\n",
    "        \"tornet_2022.tar.gz\",\n",
    "        \"catalog.csv\"\n",
    "    ]\n",
    "\n",
    "    # Construct the public URLs\n",
    "    links = [f\"https://{CUSTOM_ENDPOINT}/{APP}/{file_name}\" for file_name in file_list]\n",
    "    # links = [\n",
    "    #     \"https://zenodo.org/records/12655719/files/tornet_2022.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655718/files/tornet_2021.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655717/files/tornet_2020.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655716/files/tornet_2019.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655187/files/tornet_2018.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655183/files/tornet_2017.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655179/files/tornet_2016.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12655151/files/tornet_2015.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12637032/files/tornet_2014.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12636522/files/tornet_2013.tar.gz\",\n",
    "    #     \"https://zenodo.org/records/12636522/files/catalog.csv\",\n",
    "    # ]\n",
    "    \n",
    "    # Filter out already downloaded files\n",
    "    links_to_download = [\n",
    "        link for link in links\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, os.path.basename(link)))\n",
    "    ]\n",
    "\n",
    "    if links_to_download:\n",
    "        download_links(links_to_download)\n",
    "    else:\n",
    "        logging.info(\"All files already downloaded.\")\n",
    "\n",
    "\n",
    "def extract_local_tar_files():\n",
    "    \"\"\"\n",
    "    Extract all .tar.gz files from the local DATA_DIR to EXTRACT_DIR.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting extraction process...\")\n",
    "    for file_name in os.listdir(DATA_DIR):\n",
    "        if file_name.endswith('.tar.gz'):\n",
    "            file_path = os.path.join(DATA_DIR, file_name)\n",
    "            logging.info(f'Extracting {file_path}...')\n",
    "            with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                tar.extractall(path=EXTRACT_DIR)\n",
    "            logging.info(f'Extracted {file_path} to {EXTRACT_DIR}')\n",
    "\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "    download_files_with_aria()\n",
    "\n",
    "# Call the function to process the local .tar.gz files\n",
    "extract_local_tar_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144720c-7fdf-40fb-8e52-a79aed2e4ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733337261.010706   65628 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79197 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1733337262.922636   67384 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /opt/conda/lib/python3.11/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /opt/conda/lib/python3.11/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /opt/conda/lib/python3.11/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "W0000 00:00:1733337262.927472   67383 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.928343   67379 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.929390   67380 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.930236   67384 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.931298   67378 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.932149   67386 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.933204   67385 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "W0000 00:00:1733337262.934264   67382 gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "DATA_DIR = './data'  # Directory containing .tar.gz files\n",
    "EXTRACT_DIR = os.path.join(DATA_DIR, 'extracted')\n",
    "\n",
    "# Constants\n",
    "VARIABLES = ['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH']\n",
    "TIME_STEPS = 3\n",
    "SWEEPS = 2\n",
    "IMAGE_HEIGHT = 120\n",
    "IMAGE_WIDTH = 240\n",
    "\n",
    "# Catalog and label mapping\n",
    "catalog_path = os.path.join(EXTRACT_DIR, \"catalog.csv\")\n",
    "catalog = pd.read_csv(catalog_path)\n",
    "catalog['label'] = catalog['category'].apply(lambda x: 1 if x == 'TOR' else 0)\n",
    "label_mapping = dict(zip(catalog['filename'], catalog['label']))\n",
    "\n",
    "# Define preprocessing function for NetCDF files\n",
    "def parse_nc_file(file_path):\n",
    "    \"\"\"\n",
    "    Parse a single NetCDF file to extract radar data and label.\n",
    "    \"\"\"\n",
    "    file_path = file_path.numpy().decode('utf-8')  # Convert Tensor to string\n",
    "    with xr.open_dataset(file_path, engine=\"netcdf4\") as ds:\n",
    "        data = np.empty((IMAGE_HEIGHT, IMAGE_WIDTH, len(VARIABLES), SWEEPS, TIME_STEPS), dtype=np.float32)\n",
    "        for t in range(TIME_STEPS):\n",
    "            for s in range(SWEEPS):\n",
    "                for idx, var in enumerate(VARIABLES):\n",
    "                    channel_data = ds[var].isel(time=t, sweep=s).values\n",
    "                    np.nan_to_num(channel_data, copy=False, nan=0, posinf=0, neginf=0)\n",
    "                    data[:, :, idx, s, t] = channel_data  # Keep separate dimensions\n",
    "\n",
    "    # Transpose to match model input shape: (HEIGHT, WIDTH, VARIABLES * SWEEPS, TIME_STEPS)\n",
    "    data = data.transpose(0, 1, 2, 3, 4).reshape(IMAGE_HEIGHT, IMAGE_WIDTH, len(VARIABLES) * SWEEPS, TIME_STEPS)\n",
    "    label = label_mapping.get(os.path.basename(file_path), 0)  # Get label\n",
    "    return data, label\n",
    "\n",
    "def load_and_preprocess(file_path):\n",
    "    \"\"\"\n",
    "    Wrapper for tf.data to process NetCDF files.\n",
    "    \"\"\"\n",
    "    data, label = tf.py_function(\n",
    "        func=parse_nc_file,\n",
    "        inp=[file_path],\n",
    "        Tout=(tf.float32, tf.int32)\n",
    "    )\n",
    "    data.set_shape((IMAGE_HEIGHT, IMAGE_WIDTH, len(VARIABLES) * SWEEPS, TIME_STEPS))\n",
    "    label.set_shape(())\n",
    "    return data, label\n",
    "\n",
    "def create_tf_dataset(directory, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create an optimized TensorFlow dataset from NetCDF files in the directory.\n",
    "    \"\"\"\n",
    "    # List all NetCDF files in the directory\n",
    "    file_paths = tf.data.Dataset.list_files(f\"{directory}/**/*.nc\", shuffle=True)\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    dataset = file_paths.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch early, shuffle, and prefetch\n",
    "    dataset = dataset.batch(batch_size).shuffle(buffer_size=10000).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "X_train = create_tf_dataset(os.path.join(EXTRACT_DIR, \"train\"), batch_size=32)\n",
    "X_test = create_tf_dataset(os.path.join(EXTRACT_DIR, \"test\"), batch_size=32)\n",
    "\n",
    "for data, label in X_train.take(1):\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db146851-3d91-4769-8d35-3f9c4b4e5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_3d_torcnn(input_shape=(120, 240, 36, 3), dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Define a 3D CNN model for tornado detection.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),        layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 1)),  # Pool across spatial dimensions only\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),  # Pool across spatial and temporal dimensions\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        # Block 3\n",
    "        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        # Block 4\n",
    "        layers.Conv3D(256, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        # Output Layer\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=BinaryCrossentropy(),\n",
    "        metrics=['accuracy', Precision(), Recall(), AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create Model\n",
    "# Note: Add the temporal dimension to the input shape (TIME_STEPS = 3).\n",
    "input_shape = (120, 240, len(VARIABLES) * SWEEPS, TIME_STEPS)\n",
    "model = create_3d_torcnn(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cc128-7e0c-4647-831b-82aa74ef4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0005\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce learning rate by a factor of 0.5\n",
    "    patience=3,  # Wait 3 epochs of no improvement before reducing\n",
    "    min_lr=1e-6,  # Lower bound for the learning rate\n",
    "    verbose=1  # Print updates when learning rate is reduced\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    X_train,  # Training dataset with features and labels\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=X_test,  # Validation dataset with features and labels\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "print(\"Evaluating the model...\")\n",
    "results = model.evaluate(X_test)\n",
    "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
    "\n",
    "# Extract Features and Labels for Detailed Metrics\n",
    "X_test_features = []\n",
    "y_test_labels = []\n",
    "\n",
    "for features, labels in X_test:\n",
    "    X_test_features.append(features.numpy())\n",
    "    y_test_labels.append(labels.numpy())\n",
    "\n",
    "X_test_features = np.concatenate(X_test_features, axis=0)\n",
    "y_test_labels = np.concatenate(y_test_labels, axis=0)\n",
    "\n",
    "# Predictions\n",
    "y_pred = (model.predict(X_test_features) > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_labels, model.predict(X_test_features))\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='ROC Curve')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
