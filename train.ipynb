{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463ace2d-252f-43bb-ad84-4867da5caf18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Collecting aria2\n",
      "  Downloading aria2-0.0.1b0-py3-none-manylinux_2_17_x86_64.whl.metadata (28 kB)\n",
      "Collecting netCDF4\n",
      "  Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Collecting xarray\n",
      "  Downloading xarray-2024.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.8.30)\n",
      "Collecting cftime (from netCDF4)\n",
      "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.1)\n",
      "Collecting pandas>=2.1 (from xarray)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.1->xarray)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.1->xarray)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n",
      "Downloading aria2-0.0.1b0-py3-none-manylinux_2_17_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2024.11.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, scipy, joblib, cftime, aria2, scikit-learn, pandas, netCDF4, xarray\n",
      "Successfully installed aria2-0.0.1b0 cftime-1.6.4.post1 joblib-1.4.2 netCDF4-1.7.2 pandas-2.2.3 pytz-2024.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0 tqdm-4.67.1 tzdata-2024.2 xarray-2024.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests aria2 netCDF4 numpy xarray scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1826bbf-3b5e-4881-8d5a-fb5e1cf07799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Constants\n",
    "DOWNLOAD_DATA = True\n",
    "DATA_DIR = './data'  # Directory containing .tar.gz files\n",
    "EXTRACT_DIR = os.path.join(DATA_DIR, 'extracted')\n",
    "\n",
    "# Temporary file for download links\n",
    "TMP_FILE = os.path.join(DATA_DIR, 'tmp.txt')\n",
    "EXTRACT_DIR = os.path.join(DATA_DIR, 'extracted')\n",
    "\n",
    "# Bucket and endpoint configuration\n",
    "CUSTOM_ENDPOINT = \"bbproxy.meyerstk.com/file\"\n",
    "APP = \"TorNetBecauseZenodoSlow\"  # Bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "118c3372-4fa2-4c9c-b93d-588fc1f16403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:22:59,454 - INFO - Starting download process with aria2c...\n",
      "2024-12-05 00:22:59,456 - INFO - Temporary file created: ./data/tmp.txt\n",
      "2024-12-05 00:22:59,457 - INFO - Starting downloads for links: https://bbproxy.meyerstk.com/file/TorNetBecauseZenodoSlow/tornet_2013.tar.gz, https://bbproxy.meyerstk.com/file/TorNetBecauseZenodoSlow/catalog.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12/05 00:22:59 [\u001b[1;32mNOTICE\u001b[0m] Downloading 2 item(s)\n",
      "[DL:0B][#70f286 0B/0B][#ad92d6 0B/0B]\n",
      "[DL:0B][#70f286 0B/0B][#ad92d6 0B/0B]\n",
      "[DL:0B][#70f286 0B/0B][#ad92d6 0B/0B]\n",
      "[DL:0B][#70f286 0B/0B][#ad92d6 0B/0B]\n",
      "[DL:0B][#70f286 0B/0B][#ad92d6 0B/0B]\n",
      "[DL:1.0MiB][#70f286 496KiB/2.9GiB(0%)][#ad92d6 112KiB/36MiB(0%)]\n",
      "[DL:11MiB][#70f286 12MiB/2.9GiB(0%)][#ad92d6 6.5MiB/36MiB(17%)]\n",
      "[DL:50MiB][#70f286 110MiB/2.9GiB(3%)][#ad92d6 21MiB/36MiB(58%)]\n",
      "[DL:103MiB][#70f286 336MiB/2.9GiB(11%)][#ad92d6 35MiB/36MiB(97%)]\n",
      "\n",
      "12/05 00:23:10 [\u001b[1;32mNOTICE\u001b[0m] Download complete: ./data/catalog.csv\n",
      "[#70f286 469MiB/2.9GiB(15%) CN:16 DL:102MiB ETA:24s]\n",
      "[#70f286 825MiB/2.9GiB(27%) CN:16 DL:148MiB ETA:14s]\n",
      "[#70f286 1.0GiB/2.9GiB(36%) CN:16 DL:165MiB ETA:11s]\n",
      "[#70f286 1.3GiB/2.9GiB(44%) CN:16 DL:178MiB ETA:9s]\n",
      "[#70f286 1.5GiB/2.9GiB(53%) CN:16 DL:187MiB ETA:7s]\n",
      "[#70f286 1.7GiB/2.9GiB(60%) CN:16 DL:192MiB ETA:6s]\n",
      "[#70f286 2.0GiB/2.9GiB(68%) CN:16 DL:218MiB ETA:4s]\n",
      "[#70f286 2.2GiB/2.9GiB(77%) CN:16 DL:242MiB ETA:2s]\n",
      "[#70f286 2.4GiB/2.9GiB(84%) CN:16 DL:248MiB ETA:1s]\n",
      "[#70f286 2.7GiB/2.9GiB(91%) CN:13 DL:261MiB]\n",
      "[#70f286 2.8GiB/2.9GiB(96%) CN:9 DL:235MiB]\n",
      "[#70f286 2.9GiB/2.9GiB(99%) CN:3 DL:214MiB]\n",
      "[#70f286 2.9GiB/2.9GiB(99%) CN:1 DL:188MiB]\n",
      "\n",
      "12/05 00:23:22 [\u001b[1;31mERROR\u001b[0m] CUID#10 - Download aborted. URI=https://bbproxy.meyerstk.com/file/TorNetBecauseZenodoSlow/tornet_2013.tar.gz\n",
      "Exception: [AbstractCommand.cc:351] errorCode=8 URI=https://bbproxy.meyerstk.com/file/TorNetBecauseZenodoSlow/tornet_2013.tar.gz\n",
      "  -> [HttpResponse.cc:81] errorCode=8 Invalid range header. Request: 1774761718-1778384895/3159866899, Response: 0-3159866898/3159866899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:23:59,931 - INFO - Downloads completed successfully.\n",
      "2024-12-05 00:23:59,933 - INFO - Temporary file deleted: ./data/tmp.txt\n",
      "2024-12-05 00:23:59,935 - INFO - Starting extraction process...\n",
      "2024-12-05 00:23:59,937 - INFO - Extracting ./data/tornet_2013.tar.gz...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12/05 00:23:59 [\u001b[1;32mNOTICE\u001b[0m] Download complete: ./data/tornet_2013.tar.gz\n",
      "\n",
      "Download Results:\n",
      "gid   |stat|avg speed  |path/URI\n",
      "======+====+===========+=======================================================\n",
      "ad92d6|OK  |    10MiB/s|./data/catalog.csv\n",
      "70f286|OK  |   174MiB/s|./data/tornet_2013.tar.gz\n",
      "\n",
      "Status Legend:\n",
      "(OK):download completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:24:12,499 - INFO - Extracted ./data/tornet_2013.tar.gz to ./data/extracted\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "import tarfile\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def download_links(links):\n",
    "    \"\"\"\n",
    "    Download files from the provided links using aria2c.\n",
    "    Uses a file named tmp.txt in DATA_DIR for links.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Write links to tmp.txt\n",
    "        with open(TMP_FILE, 'w') as file:\n",
    "            file.writelines(link + '\\n' for link in links)\n",
    "        logging.info(f\"Temporary file created: {TMP_FILE}\")\n",
    "\n",
    "        # Run aria2c to download files\n",
    "        logging.info(f\"Starting downloads for links: {', '.join(links)}\")\n",
    "        command = [\n",
    "            \"aria2c\",\n",
    "            \"-j\", \"5\",                # Download up to 3 files concurrently\n",
    "            \"-x\", \"16\",               # Use up to 16 connections per file\n",
    "            # \"--console-log-level=info\",\n",
    "            \"-s\", \"16\",               # Split each file into 16 segments\n",
    "            \"--dir\", DATA_DIR,        # Specify the download directory\n",
    "            \"-i\", TMP_FILE            # Input file with download links\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n",
    "        logging.info(\"Downloads completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during download: {e}\")\n",
    "        exit(1)\n",
    "    finally:\n",
    "        if os.path.exists(TMP_FILE):\n",
    "            os.remove(TMP_FILE)\n",
    "            logging.info(f\"Temporary file deleted: {TMP_FILE}\")\n",
    "\n",
    "\n",
    "def download_files_with_aria():\n",
    "    \"\"\"\n",
    "    Download files from a public Backblaze B2 bucket served via a custom endpoint using aria2c.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting download process with aria2c...\")\n",
    "\n",
    "    # # List of files to download\n",
    "    file_list = [\n",
    "        \"tornet_2013.tar.gz\",\n",
    "        # \"tornet_2014.tar.gz\",\n",
    "        # \"tornet_2015.tar.gz\",\n",
    "        # \"tornet_2016.tar.gz\",\n",
    "        # \"tornet_2017.tar.gz\",\n",
    "        # \"tornet_2018.tar.gz\",\n",
    "        # \"tornet_2019.tar.gz\",\n",
    "        # \"tornet_2020.tar.gz\",\n",
    "        # \"tornet_2021.tar.gz\",\n",
    "        # \"tornet_2022.tar.gz\",\n",
    "        \"catalog.csv\"\n",
    "    ]\n",
    "\n",
    "    # Construct the public URLs\n",
    "    links = [f\"https://{CUSTOM_ENDPOINT}/{APP}/{file_name}\" for file_name in file_list]\n",
    "\n",
    "    links = [\n",
    "        \"https://zenodo.org/records/12636522/files/catalog.csv\",\n",
    "        \"https://zenodo.org/records/12636522/files/tornet_2013.tar.gz\",\n",
    "    ]\n",
    "    \n",
    "    # Filter out already downloaded files\n",
    "    links_to_download = [\n",
    "        link for link in links\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, os.path.basename(link)))\n",
    "    ]\n",
    "\n",
    "    if links_to_download:\n",
    "        download_links(links_to_download)\n",
    "    else:\n",
    "        logging.info(\"All files already downloaded.\")\n",
    "\n",
    "\n",
    "def extract_local_tar_files():\n",
    "    \"\"\"\n",
    "    Extract all .tar.gz files from the local DATA_DIR to EXTRACT_DIR.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting extraction process...\")\n",
    "    for file_name in os.listdir(DATA_DIR):\n",
    "        if file_name.endswith('.tar.gz'):\n",
    "            file_path = os.path.join(DATA_DIR, file_name)\n",
    "            logging.info(f'Extracting {file_path}...')\n",
    "            with tarfile.open(file_path, 'r:gz') as tar:\n",
    "                tar.extractall(path=EXTRACT_DIR)\n",
    "            logging.info(f'Extracted {file_path} to {EXTRACT_DIR}')\n",
    "\n",
    "# if DOWNLOAD_DATA:\n",
    "#     download_files_with_aria()\n",
    "\n",
    "# Call the function to process the local .tar.gz files\n",
    "extract_local_tar_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c75a6f-cec6-41a6-80ad-ed1ab246f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing year 2013 with 3498 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ./data/tfrecords/train/2013.tfrecord:  86%|████████▌ | 2993/3498 [02:10<00:21, 23.06it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = './data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "TFRECORD_DIR = os.path.join(DATA_DIR, 'tfrecords')\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "CHANNEL_MIN_MAX = {\n",
    "    'DBZ': [-20., 60.],\n",
    "    'VEL': [-60., 60.],\n",
    "    'KDP': [-2., 5.],\n",
    "    'RHOHV': [0.2, 1.04],\n",
    "    'ZDR': [-1., 8.],\n",
    "    'WIDTH': [0., 9.]\n",
    "}\n",
    "\n",
    "# Ensure TFRecord directory exists\n",
    "os.makedirs(TFRECORD_DIR, exist_ok=True)\n",
    "\n",
    "# Parsing and Preprocessing Functions\n",
    "def parse_nc_file(file_path):\n",
    "    \"\"\"\n",
    "    Parse and preprocess a single .nc file into normalized data and labels.\n",
    "    \"\"\"\n",
    "    if not isinstance(file_path, str):\n",
    "        file_path = file_path.numpy().decode(\"utf-8\")  # Convert TensorFlow tensor to string\n",
    "\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, engine=\"netcdf4\") as ds:\n",
    "            variables = ['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH']\n",
    "            data_list = []\n",
    "            for var in variables:\n",
    "                var_data = ds[var].values  # Shape: [time, azimuth, range, sweep]\n",
    "                var_min, var_max = CHANNEL_MIN_MAX[var]\n",
    "                var_data = np.nan_to_num(var_data, nan=0, posinf=0, neginf=0)  # Replace NaNs\n",
    "                var_data = np.clip(var_data, var_min, var_max)  # Clip to range\n",
    "                var_data = (var_data - var_min) / (var_max - var_min)  # Normalize to [0, 1]\n",
    "                data_list.append(var_data)\n",
    "\n",
    "            # Concatenate variables along the channel dimension\n",
    "            data = np.stack(data_list, axis=-1)  # Shape: [time, azimuth, range, sweep, num_variables]\n",
    "\n",
    "            # Reshape to combine sweep and variable dimensions into one channel dimension\n",
    "            time, azimuth, range_, sweep, num_variables = data.shape\n",
    "            num_channels = num_variables * sweep\n",
    "            data = data.reshape(time, azimuth, range_, num_channels)\n",
    "\n",
    "            # Extract the label\n",
    "            label = ds.attrs.get(\"category\", \"NUL\")\n",
    "            label = 1 if label == \"TOR\" else 0\n",
    "\n",
    "            return data, label\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def serialize_example(data, label):\n",
    "    \"\"\"\n",
    "    Serialize data and label into TFRecord format.\n",
    "    \"\"\"\n",
    "    feature = {\n",
    "        \"feature\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data.tobytes()])),\n",
    "        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
    "\n",
    "def preprocess_to_tfrecord(files, output_path):\n",
    "    \"\"\"\n",
    "    Preprocess a list of .nc files and save them as a single TFRecord file.\n",
    "    \"\"\"\n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        for file in tqdm(files, desc=f\"Processing {output_path}\"):\n",
    "            try:\n",
    "                file_path = str(file)  # Convert PosixPath to string\n",
    "                data, label = parse_nc_file(file_path)\n",
    "                if data is not None:\n",
    "                    serialized_example = serialize_example(data, label)\n",
    "                    writer.write(serialized_example)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file}: {e}\")\n",
    "\n",
    "def create_tfrecords(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Create TFRecord files from .nc files organized by year.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    years = [p for p in Path(input_dir).iterdir() if p.is_dir()]\n",
    "\n",
    "    for year_dir in years:\n",
    "        year = year_dir.name\n",
    "        files = list(year_dir.rglob(\"*.nc\"))\n",
    "        files = [str(f) for f in files]  # Convert PosixPath to string\n",
    "        output_path = os.path.join(output_dir, f\"{year}.tfrecord\")\n",
    "\n",
    "        print(f\"Processing year {year} with {len(files)} files...\")\n",
    "        preprocess_to_tfrecord(files, output_path)\n",
    "\n",
    "# Dataset Loading\n",
    "def parse_tfrecord(serialized_example):\n",
    "    \"\"\"\n",
    "    Parse a serialized TFRecord example.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        \"feature\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    data = tf.io.decode_raw(example[\"feature\"], tf.float32)\n",
    "    data = tf.reshape(data, [4, 120, 240, 12])  # Shape derived from input dimensions\n",
    "    label = example[\"label\"]\n",
    "    return data, label\n",
    "\n",
    "def permute_axes(features, label):\n",
    "    \"\"\"\n",
    "    Rearrange axes of the features to match model input shape.\n",
    "    Input: (time, azimuth, range, channels)\n",
    "    Output: (azimuth, range, channels, time)\n",
    "    \"\"\"\n",
    "    features = tf.transpose(features, perm=[1, 2, 3, 0])  # Move 'time' to the last axis\n",
    "    return features, label\n",
    "\n",
    "def create_tf_dataset_from_tfrecord(tfrecord_dir, batch_size=32, buffer_size=10000):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset from TFRecord files with enhanced parallelism.\n",
    "    \"\"\"\n",
    "    tfrecord_files = tf.data.Dataset.list_files(f\"{tfrecord_dir}/*.tfrecord\", shuffle=True)\n",
    "    dataset = tfrecord_files.interleave(\n",
    "        lambda x: tf.data.TFRecordDataset(x),\n",
    "        cycle_length=16,  # Number of parallel files to read\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(permute_axes, num_parallel_calls=tf.data.AUTOTUNE)  # Adjust axis order\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "create_tfrecords(TRAIN_DIR, train_tfrecord_dir)\n",
    "\n",
    "print(\"Processing testing data...\")\n",
    "create_tfrecords(TEST_DIR, test_tfrecord_dir)\n",
    "\n",
    "# Step 2: Load datasets from TFRecords\n",
    "print(\"Loading training dataset...\")\n",
    "train_dataset = create_tf_dataset_from_tfrecord(train_tfrecord_dir, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Loading testing dataset...\")\n",
    "test_dataset = create_tf_dataset_from_tfrecord(test_tfrecord_dir, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Step 3: Validate the pipeline\n",
    "print(\"Validating dataset...\")\n",
    "for data, label in test_dataset.take(1):\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db146851-3d91-4769-8d35-3f9c4b4e5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_3d_torcnn(input_shape=(120, 240, 36, 3), dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Define a 3D CNN model for tornado detection.\n",
    "    \"\"\"\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            # Block 1\n",
    "            layers.Input(shape=input_shape),\n",
    "            layers.Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D((2, 2, 1)),  # Pool across spatial dimensions only\n",
    "            layers.Dropout(dropout_rate),\n",
    "            # Block 2\n",
    "            layers.Conv3D(64, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.Conv3D(64, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D(\n",
    "                (2, 2, 2)\n",
    "            ),  # Pool across spatial and temporal dimensions\n",
    "            layers.Dropout(dropout_rate),\n",
    "            # Block 3\n",
    "            layers.Conv3D(128, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.Conv3D(128, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D((2, 2, 2)),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            # Block 4\n",
    "            layers.Conv3D(256, (3, 3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling3D((2, 2, 2)),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            # Fully Connected Layers\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            # Output Layer\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=BinaryCrossentropy(),\n",
    "        metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create Model\n",
    "# Note: Add the temporal dimension to the input shape (TIME_STEPS = 3).\n",
    "input_shape = (120, 240, len(VARIABLES) * SWEEPS, TIME_STEPS)\n",
    "model = create_3d_torcnn(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347cc128-7e0c-4647-831b-82aa74ef4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv3d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 4, 120, 240, 12)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 4, 120, 240, 12), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Start Training\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training dataset with features and labels\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation dataset with features and labels\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Evaluate the Model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv3d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 4, 120, 240, 12)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 4, 120, 240, 12), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0005\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce learning rate by a factor of 0.5\n",
    "    patience=3,  # Wait 3 epochs of no improvement before reducing\n",
    "    min_lr=1e-6,  # Lower bound for the learning rate\n",
    "    verbose=1  # Print updates when learning rate is reduced\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    train_dataset,  # Training dataset with features and labels\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_dataset,  # Validation dataset with features and labels\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "print(\"Evaluating the model...\")\n",
    "results = model.evaluate(X_test)\n",
    "print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\")\n",
    "\n",
    "# Extract Features and Labels for Detailed Metrics\n",
    "X_test_features = []\n",
    "y_test_labels = []\n",
    "\n",
    "for features, labels in X_test:\n",
    "    X_test_features.append(features.numpy())\n",
    "    y_test_labels.append(labels.numpy())\n",
    "\n",
    "X_test_features = np.concatenate(X_test_features, axis=0)\n",
    "y_test_labels = np.concatenate(y_test_labels, axis=0)\n",
    "\n",
    "# Predictions\n",
    "y_pred = (model.predict(X_test_features) > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_labels, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_labels, model.predict(X_test_features))\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='ROC Curve')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b13ab6-fe10-453f-a31a-c95df10e9052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
